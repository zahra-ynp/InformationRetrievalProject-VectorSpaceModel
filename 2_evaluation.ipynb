{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae88e42",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5fb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_artifacts():\n",
    "    \"\"\"Loads the processed data artifacts from disk.\"\"\"\n",
    "    print(\"--- Loading system artifacts... ---\")\n",
    "    try:\n",
    "        with open(\"./artifacts/cranfield_vectors.pkl\", 'rb') as f:\n",
    "            doc_vectors = pickle.load(f)\n",
    "\n",
    "        with open(\"./artifacts/cranfield_idf.pkl\", 'rb') as f:\n",
    "            idf_scores = pickle.load(f)\n",
    "            \n",
    "        print(\"Artifacts loaded successfully.\")\n",
    "        return doc_vectors, idf_scores\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Could not find artifact files. Please run the '1_Index_Builder.ipynb' first.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad4ad3",
   "metadata": {},
   "source": [
    "Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2faeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(filepath):\n",
    "    \"\"\"\n",
    "    Parses the Cranfield queries file (.qry) to extract query IDs and their text.\n",
    "\n",
    "    The function reads the file line by line, identifying new queries by the '.I' marker\n",
    "    and accumulating the query text that follows the '.W' marker.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the final query data\n",
    "    queries = {}\n",
    "    \n",
    "    # Use -1 as a placeholder to indicate that we haven't started reading the first query yet.\n",
    "    current_id = -1\n",
    "    \n",
    "    # Initialize an empty string to accumulate the text for the query being processed.\n",
    "    current_text = \"\"\n",
    "    \n",
    "    # Open the specified file for reading.\n",
    "    with open(filepath, 'r') as f:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in f:\n",
    "            # Check if the line marks the beginning of a new query\n",
    "            if line.startswith('.I'):\n",
    "                # If current_id is not -1, it means we have just finished reading a\n",
    "                # previous query, and its text needs to be saved before we start the new one.\n",
    "                if current_id != -1:\n",
    "                    # Save the accumulated text. .strip() removes whitespace from the ends,\n",
    "                    # and .replace() handles text that spanned multiple lines.\n",
    "                    queries[current_id] = current_text.strip().replace('\\n', ' ')\n",
    "                \n",
    "                # Extract the new query ID from the line\n",
    "                current_id = int(line.split()[1])\n",
    "                \n",
    "                # Reset the text accumulator to start fresh for this new query.\n",
    "                current_text = \"\"\n",
    "                \n",
    "            # Check if the line is the '.W' marker, which indicates the start of the text.\n",
    "            elif line.startswith('.W'):\n",
    "                # This line is just a marker, so we don't need to do anything with it.\n",
    "                pass\n",
    "                \n",
    "            # If the line is not a marker, it must be part of the query text.\n",
    "            else:\n",
    "                # Append the line to our accumulator for the current query.\n",
    "                current_text += line\n",
    "                \n",
    "    # --- Final save after the loop ---\n",
    "    # When the loop finishes, the text for the very last query is still in 'current_text'.\n",
    "    # This final block ensures that the last query is also added to the dictionary.\n",
    "    if current_id != -1:\n",
    "        queries[current_id] = current_text.strip().replace('\\n', ' ')\n",
    "        \n",
    "    # Return the completed dictionary of queries.\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed6ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries =load_queries(\"./dataset/cran.qry\")\n",
    "    \n",
    "# Print the first 3 queries to see if they look correct\n",
    "for i in range(1, 5):\n",
    "    if i in queries:\n",
    "        print(f\"Query ID {i}: {queries[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b876341",
   "metadata": {},
   "source": [
    "Load relevance judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26400e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_relevance_judgments(filepath):\n",
    "    \"\"\"\n",
    "    Parses the cranqrel file to load the ground truth relevance data.\n",
    "\n",
    "    The function reads the file line by line, where each line contains a\n",
    "    query ID, a relevant document ID, and a relevance score. It then\n",
    "    organizes this data into a dictionary for easy lookup.\n",
    "    \"\"\"\n",
    "    # Initialize a defaultdict with the default factory 'list'.\n",
    "    qrels = defaultdict(list)\n",
    "    \n",
    "    # Open the specified file \n",
    "    with open(filepath, 'r') as f:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in f:\n",
    "            # Each line in cranqrel looks like: \"1 184 2\"\n",
    "            # .split() will turn this string into a list of strings: [\"1\", \"184\", \"2\"]\n",
    "            # map(int, ...) applies the int() function to each item in that list,\n",
    "            # converting them into integers.\n",
    "            # We then unpack these three integers into three variables.\n",
    "            # The underscore '_' is a convention for a variable we don't plan to use.\n",
    "            query_id, doc_id, _ = map(int, line.split())\n",
    "            \n",
    "            # Use the query_id as the key and append the relevant doc_id to its list.\n",
    "            qrels[query_id].append(doc_id)\n",
    "            \n",
    "    # Return the completed dictionary of relevance judgments.\n",
    "    return qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c430c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 225 queries and relevance judgments for 225 queries.\n"
     ]
    }
   ],
   "source": [
    "relevance_data = load_relevance_judgments(\"./dataset/cranqrel\")\n",
    "\n",
    "print(f\"\\nLoaded {len(queries)} queries and relevance judgments for {len(relevance_data)} queries.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
