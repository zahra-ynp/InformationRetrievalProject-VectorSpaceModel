{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae88e42",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5fb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_artifacts():\n",
    "    \"\"\"Loads the processed data artifacts from disk.\"\"\"\n",
    "    print(\"--- Loading system artifacts... ---\")\n",
    "    try:\n",
    "        with open(\"./artifacts/cranfield_vectors.pkl\", 'rb') as f:\n",
    "            doc_vectors = pickle.load(f)\n",
    "\n",
    "        with open(\"./artifacts/cranfield_idf.pkl\", 'rb') as f:\n",
    "            idf_scores = pickle.load(f)\n",
    "            \n",
    "        print(\"Artifacts loaded successfully.\")\n",
    "        return doc_vectors, idf_scores\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Could not find artifact files. Please run the '1_Index_Builder.ipynb' first.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors, idf_scores =load_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad4ad3",
   "metadata": {},
   "source": [
    "Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2faeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(filepath):\n",
    "    \"\"\"\n",
    "    Parses the Cranfield queries file (.qry) to extract query IDs and their text.\n",
    "\n",
    "    The function reads the file line by line, identifying new queries by the '.I' marker\n",
    "    and accumulating the query text that follows the '.W' marker.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the final query data\n",
    "    queries = {}\n",
    "    \n",
    "    # Use -1 as a placeholder to indicate that we haven't started reading the first query yet.\n",
    "    current_id = -1\n",
    "    \n",
    "    # Initialize an empty string to accumulate the text for the query being processed.\n",
    "    current_text = \"\"\n",
    "    \n",
    "    # Open the specified file for reading.\n",
    "    with open(filepath, 'r') as f:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in f:\n",
    "            # Check if the line marks the beginning of a new query\n",
    "            if line.startswith('.I'):\n",
    "                # If current_id is not -1, it means we have just finished reading a\n",
    "                # previous query, and its text needs to be saved before we start the new one.\n",
    "                if current_id != -1:\n",
    "                    # Save the accumulated text. .strip() removes whitespace from the ends,\n",
    "                    # and .replace() handles text that spanned multiple lines.\n",
    "                    queries[current_id] = current_text.strip().replace('\\n', ' ')\n",
    "                \n",
    "                # Extract the new query ID from the line\n",
    "                current_id = int(line.split()[1])\n",
    "                \n",
    "                # Reset the text accumulator to start fresh for this new query.\n",
    "                current_text = \"\"\n",
    "                \n",
    "            # Check if the line is the '.W' marker, which indicates the start of the text.\n",
    "            elif line.startswith('.W'):\n",
    "                # This line is just a marker, so we don't need to do anything with it.\n",
    "                pass\n",
    "                \n",
    "            # If the line is not a marker, it must be part of the query text.\n",
    "            else:\n",
    "                # Append the line to our accumulator for the current query.\n",
    "                current_text += line\n",
    "                \n",
    "    # --- Final save after the loop ---\n",
    "    # When the loop finishes, the text for the very last query is still in 'current_text'.\n",
    "    # This final block ensures that the last query is also added to the dictionary.\n",
    "    if current_id != -1:\n",
    "        queries[current_id] = current_text.strip().replace('\\n', ' ')\n",
    "        \n",
    "    # Return the completed dictionary of queries.\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed6ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries =load_queries(\"./dataset/cran.qry\")\n",
    "    \n",
    "# Print the first 3 queries to see if they look correct\n",
    "for i in range(1, 5):\n",
    "    if i in queries:\n",
    "        print(f\"Query ID {i}: {queries[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b876341",
   "metadata": {},
   "source": [
    "Load relevance judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26400e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_relevance_judgments(filepath):\n",
    "    \"\"\"\n",
    "    Parses the cranqrel file to load the ground truth relevance data.\n",
    "\n",
    "    The function reads the file line by line, where each line contains a\n",
    "    query ID, a relevant document ID, and a relevance score. It then\n",
    "    organizes this data into a dictionary for easy lookup.\n",
    "    \"\"\"\n",
    "    # Initialize a defaultdict with the default factory 'list'.\n",
    "    qrels = defaultdict(list)\n",
    "    \n",
    "    # Open the specified file \n",
    "    with open(filepath, 'r') as f:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in f:\n",
    "            # Each line in cranqrel looks like: \"1 184 2\"\n",
    "            # .split() will turn this string into a list of strings: [\"1\", \"184\", \"2\"]\n",
    "            # map(int, ...) applies the int() function to each item in that list,\n",
    "            # converting them into integers.\n",
    "            # We then unpack these three integers into three variables.\n",
    "            # The underscore '_' is a convention for a variable we don't plan to use.\n",
    "            query_id, doc_id, _ = map(int, line.split())\n",
    "            \n",
    "            # Use the query_id as the key and append the relevant doc_id to its list.\n",
    "            qrels[query_id].append(doc_id)\n",
    "            \n",
    "    # Return the completed dictionary of relevance judgments.\n",
    "    return qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c430c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_data = load_relevance_judgments(\"./dataset/cranqrel\")\n",
    "\n",
    "print(f\"\\nLoaded {len(queries)} queries and relevance judgments for {len(relevance_data)} queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bafd1de",
   "metadata": {},
   "source": [
    "Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(retrieved_docs, relevant_docs):\n",
    "    \"\"\"\n",
    "    Calculates Precision, Recall, and F1-Score for a single query.\n",
    "    \"\"\"\n",
    "    # Convert lists to sets for efficient intersection\n",
    "    retrieved_set = set(retrieved_docs)\n",
    "    relevant_set = set(relevant_docs)\n",
    "\n",
    "    # True Positives are the documents that are in both lists\n",
    "    true_positives = len(retrieved_set.intersection(relevant_set))\n",
    "\n",
    "    # --- Precision --- \n",
    "    # Measures: \"Of the documents our system returned, how many were actually relevant?\"\n",
    "    # P = (Number of relevant items retrieved) / (Total number of items retrieved)\n",
    "    precision = true_positives / len(retrieved_docs) if retrieved_docs else 0.0\n",
    "\n",
    "    # --- Recall --- \n",
    "    # Measures: \"Of all the possible relevant documents, how many did our system find?\"\n",
    "    # R = (Number of relevant items retrieved) / (Total number of relevant items)\n",
    "    recall = true_positives / len(relevant_docs) if relevant_docs else 0.0\n",
    "\n",
    "    # --- F1-Score ---\n",
    "    # The harmonic mean of Precision and Recall, balancing both metrics.\n",
    "    f1_score = 0.0\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval import rank_documents\n",
    "from retrieval import preprocess_text\n",
    "\n",
    "# A list to store the performance metrics for every query\n",
    "all_metrics = []\n",
    "\n",
    "# K is the number of documents we look at for calculating precision (e.g., Precision@10)\n",
    "K = 10 \n",
    "\n",
    "print(\"--- Starting full evaluation... ---\")\n",
    "\n",
    "# Ensure all the necessary data has been loaded before starting the loop\n",
    "if 'document_vectors' in locals() and 'queries' in locals() and 'relevance_data' in locals():\n",
    "    # Loop through each query using its ID and text from the loaded queries dictionary\n",
    "    for query_id, query_text in queries.items():\n",
    "        \n",
    "        # 1. Preprocess the query text using the same function as the documents\n",
    "        query_tokens = preprocess_text([query_text])[0]\n",
    "        \n",
    "        # 2. Get system's ranked list of documents for the processed query\n",
    "        ranked_results = rank_documents(query_tokens, document_vectors, idf_scores)\n",
    "        \n",
    "        # 3. Extract just the document IDs from results, up to the first K documents\n",
    "        retrieved_top_k_ids = [doc_id for doc_id, score in ranked_results[:K]]\n",
    "        \n",
    "        # 4. Get the official list of relevant documents (the ground truth) for this query ID\n",
    "        # .get() is used safely in case a query ID has no relevance judgments\n",
    "        ground_truth_ids = relevance_data.get(query_id, [])\n",
    "        \n",
    "        # 5. Use metric calculation function to get the performance for this single query\n",
    "        query_metrics = calculate_metrics(retrieved_top_k_ids, ground_truth_ids)\n",
    "        \n",
    "        # 6. Store the results for this query in our main list\n",
    "        all_metrics.append(query_metrics)\n",
    "\n",
    "print(f\"Evaluation complete. Processed {len(all_metrics)} queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4341a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the evaluation loop has produced any metrics\n",
    "if all_metrics:\n",
    "    # Calculate the average (mean) of each metric across all 225 queries\n",
    "    mean_precision = sum(m['precision'] for m in all_metrics) / len(all_metrics)\n",
    "    mean_recall = sum(m['recall'] for m in all_metrics) / len(all_metrics)\n",
    "    mean_f1_score = sum(m['f1_score'] for m in all_metrics) / len(all_metrics)\n",
    "\n",
    "    # Print a clean, formatted summary of the final results\n",
    "    print(\"\\n--- System Performance Summary ---\")\n",
    "    print(f\"Metrics calculated at K={K}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall:    {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1-Score:  {mean_f1_score:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"No metrics were calculated. Please ensure the evaluation loop ran correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
