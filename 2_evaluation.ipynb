{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae88e42",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5fb1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_artifacts():\n",
    "    \"\"\"Loads the processed data artifacts from disk.\"\"\"\n",
    "    print(\"--- Loading system artifacts... ---\")\n",
    "    try:\n",
    "        with open(\"./artifacts/cranfield_vectors.pkl\", 'rb') as f:  \n",
    "            doc_vectors = pickle.load(f)\n",
    "\n",
    "        with open(\"./artifacts/cranfield_idf.pkl\", 'rb') as f:\n",
    "            idf_scores = pickle.load(f)\n",
    "            \n",
    "        print(\"Artifacts loaded successfully.\")\n",
    "        return doc_vectors, idf_scores\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Could not find artifact files. Please run the '1_Index_Builder.ipynb' first.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1156230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading system artifacts... ---\n",
      "Artifacts loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "document_vectors, idf_scores =load_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad4ad3",
   "metadata": {},
   "source": [
    "Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2faeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(filepath):\n",
    "    \"\"\"\n",
    "    Parses the Cranfield queries file (.qry) to extract query IDs and their text.\n",
    "\n",
    "    The function reads the file line by line, identifying new queries by the '.I' marker\n",
    "    and accumulating the query text that follows the '.W' marker.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the final query data\n",
    "    queries = {}\n",
    "    \n",
    "    # Use -1 as a placeholder to indicate that we haven't started reading the first query yet.\n",
    "    current_id = -1\n",
    "    \n",
    "    # Initialize an empty string to accumulate the text for the query being processed.\n",
    "    current_text = \"\"\n",
    "    \n",
    "    # Open the specified file for reading.\n",
    "    with open(filepath, 'r') as f:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in f:\n",
    "            # Check if the line marks the beginning of a new query\n",
    "            if line.startswith('.I'):\n",
    "                # If current_id is not -1, it means we have just finished reading a\n",
    "                # previous query, and its text needs to be saved before we start the new one.\n",
    "                if current_id != -1:\n",
    "                    # Save the accumulated text. .strip() removes whitespace from the ends\n",
    "                    queries[current_id] = current_text.strip()\n",
    "                \n",
    "                # Extract the new query ID from the line\n",
    "                current_id = int(line.split()[1])\n",
    "                \n",
    "                # Reset the text accumulator to start fresh for this new query.\n",
    "                current_text = \"\"\n",
    "                \n",
    "            # Check if the line is the '.W' marker, which indicates the start of the text.\n",
    "            elif line.startswith('.W'):\n",
    "                # This line is just a marker, so we don't need to do anything with it.\n",
    "                pass\n",
    "                \n",
    "            # If the line is not a marker, it must be part of the query text.\n",
    "            else:\n",
    "                # Append the line to our accumulator for the current query.\n",
    "                current_text += line\n",
    "                \n",
    "    # --- Final save after the loop ---\n",
    "    # When the loop finishes, the text for the very last query is still in 'current_text'.\n",
    "    # This final block ensures that the last query is also added to the dictionary.\n",
    "    if current_id != -1:\n",
    "        queries[current_id] = current_text.strip()\n",
    "        \n",
    "    # Return the completed dictionary of queries.\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beed6ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID 1: what similarity laws must be obeyed when constructing aeroelastic models\n",
      "of heated high speed aircraft .\n",
      "Query ID 2: what are the structural and aeroelastic problems associated with flight\n",
      "of high speed aircraft .\n",
      "Query ID 4: what problems of heat conduction in composite slabs have been solved so\n",
      "far .\n"
     ]
    }
   ],
   "source": [
    "queries =load_queries(\"./dataset/cran.qry\")\n",
    "    \n",
    "# Print the first 3 queries to see if they look correct\n",
    "for i in range(1, 5):\n",
    "    if i in queries:\n",
    "        print(f\"Query ID {i}: {queries[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b876341",
   "metadata": {},
   "source": [
    "Load relevance judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26400e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_relevance_judgments(filepath):\n",
    "    \"\"\"\n",
    "    Parses the cranqrel file to load the ground truth relevance data.\n",
    "\n",
    "    The function reads the file line by line, where each line contains a\n",
    "    query ID, a relevant document ID, and a relevance score. It then\n",
    "    organizes this data into a dictionary for easy lookup.\n",
    "    \"\"\"\n",
    "    # Initialize a defaultdict with the default factory 'list'.\n",
    "    qrels = defaultdict(list)\n",
    "    \n",
    "    # Open the specified file \n",
    "    with open(filepath, 'r') as f:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in f:\n",
    "            # Each line in cranqrel looks like: \"1 184 2\"\n",
    "            # .split() will turn this string into a list of strings: [\"1\", \"184\", \"2\"]\n",
    "            # map(int, ...) applies the int() function to each item in that list,\n",
    "            # converting them into integers.\n",
    "            # We then unpack these three integers into three variables.\n",
    "            query_id, doc_id, relecance_score = map(int, line.split())\n",
    "            \n",
    "            if relecance_score > 0:\n",
    "                # Use the query_id as the key and append the relevant doc_id to its list.\n",
    "                 qrels[query_id].append(doc_id)\n",
    "                \n",
    "    # Return the completed dictionary of relevance judgments.\n",
    "    return qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c430c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 225 queries and relevance judgments for 225 queries.\n"
     ]
    }
   ],
   "source": [
    "relevance_data = load_relevance_judgments(\"./dataset/cranqrel\")\n",
    "\n",
    "print(f\"\\nLoaded {len(queries)} queries and relevance judgments for {len(relevance_data)} queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bafd1de",
   "metadata": {},
   "source": [
    "Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6481c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(retrieved_docs, relevant_docs):\n",
    "    \"\"\"\n",
    "    Calculates Precision, Recall, and F1-Score for a single query.\n",
    "    \"\"\"\n",
    "    # Convert lists to sets for efficient intersection\n",
    "    retrieved_set = set(retrieved_docs)\n",
    "    relevant_set = set(relevant_docs)\n",
    "\n",
    "    # True Positives are the documents that are in both lists\n",
    "    true_positives = len(retrieved_set.intersection(relevant_set))\n",
    "\n",
    "    # --- Precision --- \n",
    "    # Measures: \"Of the documents our system returned, how many were actually relevant?\"\n",
    "    # P = (Number of relevant items retrieved) / (Total number of items retrieved)\n",
    "    precision = true_positives / len(retrieved_docs) if retrieved_docs else 0.0\n",
    "\n",
    "    # --- Recall --- \n",
    "    # Measures: \"Of all the possible relevant documents, how many did our system find?\"\n",
    "    # R = (Number of relevant items retrieved) / (Total number of relevant items)\n",
    "    recall = true_positives / len(relevant_docs) if relevant_docs else 0.0\n",
    "\n",
    "    # --- F1-Score ---\n",
    "    # The harmonic mean of Precision and Recall, balancing both metrics.\n",
    "    f1_score = 0.0\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "260678c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting full evaluation... ---\n",
      "Evaluation complete. Processed 225 queries.\n"
     ]
    }
   ],
   "source": [
    "# Run Full System Evaluation\n",
    "\n",
    "from retrieval import preprocess_text, create_query_vector ,rank_documents\n",
    "K = 10\n",
    "all_metrics = []\n",
    "\n",
    "print(\"--- Starting full evaluation... ---\")\n",
    "\n",
    "if document_vectors and queries and relevance_data:\n",
    "    for query_id, query_text in queries.items():\n",
    "        \n",
    "        # --- Initial Query ---\n",
    "        query_tokens = preprocess_text([query_text])[0]\n",
    "\n",
    "        original_query_vector = create_query_vector(query_tokens, idf_scores)\n",
    "\n",
    "        # --- Get Initial Ranking ---\n",
    "        ranked_results = rank_documents(original_query_vector, document_vectors)\n",
    "\n",
    "\n",
    "        # --- Calculate Metrics on the FINAL ranking ---\n",
    "        retrieved_top_k_ids = [doc_id for doc_id, score in ranked_results[:K]]\n",
    "        ground_truth_ids = relevance_data.get(query_id, [])\n",
    "        query_metrics = calculate_metrics(retrieved_top_k_ids, ground_truth_ids)\n",
    "        all_metrics.append(query_metrics)\n",
    "\n",
    "print(f\"Evaluation complete. Processed {len(all_metrics)} queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c57aee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- System Performance Summary ---\n",
      "------------------------------\n",
      "Mean Precision: 0.0049\n",
      "Mean Recall:    0.0029\n",
      "Mean F1-Score:  0.0035\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check if the evaluation loop has produced any metrics\n",
    "if all_metrics:\n",
    "    # Calculate the average (mean) of each metric across all 225 queries\n",
    "    mean_precision = sum(m['precision'] for m in all_metrics) / len(all_metrics)\n",
    "    mean_recall = sum(m['recall'] for m in all_metrics) / len(all_metrics)\n",
    "    mean_f1_score = sum(m['f1_score'] for m in all_metrics) / len(all_metrics)\n",
    "\n",
    "    # Print a clean, formatted summary of the final results\n",
    "    print(\"\\n--- System Performance Summary ---\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall:    {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1-Score:  {mean_f1_score:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"No metrics were calculated. Please ensure the evaluation loop ran correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9ae0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_query(query_id, k, queries, relevance_data, document_vectors, idf_scores):\n",
    "    \"\"\"\n",
    "    Runs the full retrieval and evaluation process for a single query ID\n",
    "    and prints a detailed summary of the results.\n",
    "\n",
    "    This is because our relevance judgments are not very good, for some queries we have for example\n",
    "    4 documents that are relevant, and for another one it can be more than 10\n",
    "    \"\"\"\n",
    "    # --- Check if the query exists ---\n",
    "    if query_id not in queries:\n",
    "        print(f\"Error: Query ID {query_id} not found.\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Get Query and Ground Truth ---\n",
    "    query_text = queries[query_id]\n",
    "    ground_truth_ids = relevance_data.get(query_id, [])\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Evaluating Query ID: {query_id}\")\n",
    "    print(f\"Query Text: '{query_text}'\")\n",
    "    print(f\"Ground Truth Relevant Docs: {ground_truth_ids}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # --- 2. Process and Rank ---\n",
    "    query_tokens = preprocess_text([query_text])[0]\n",
    "    query_vector = create_query_vector(query_tokens, idf_scores)\n",
    "    ranked_results = rank_documents(query_vector, document_vectors)\n",
    "\n",
    "    # --- 3. Get Retrieved Docs and Calculate Metrics ---\n",
    "    retrieved_top_k_ids = [doc_id for doc_id, score in ranked_results[:k]]\n",
    "    \n",
    "    # We already have a function for this!\n",
    "    metrics = calculate_metrics(retrieved_top_k_ids, ground_truth_ids)\n",
    "\n",
    "    # --- 4. Print Results ---\n",
    "    print(f\"Top {k} Retrieved Documents: {retrieved_top_k_ids}\\n\")\n",
    "    \n",
    "    print(\"--- Performance Metrics ---\")\n",
    "    print(f\"Precision@{k}:   {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall@{k}:      {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score@{k}:    {metrics['f1_score']:.4f}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0a6e12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Evaluating Query ID: 2\n",
      "Query Text: 'what are the structural and aeroelastic problems associated with flight\n",
      "of high speed aircraft .'\n",
      "Ground Truth Relevant Docs: [12, 15, 184, 858, 51, 102, 202, 14, 52, 380, 746, 859, 948, 285, 390, 391, 442, 497, 643, 856, 857, 877, 864, 658]\n",
      "==================================================\n",
      "Top 50 Retrieved Documents: [12, 1167, 51, 253, 791, 605, 809, 699, 1161, 1377, 745, 141, 100, 429, 1040, 875, 1145, 803, 316, 1261, 1142, 1087, 573, 908, 924, 496, 724, 1163, 1195, 1378, 883, 1166, 882, 415, 1165, 1297, 453, 1109, 76, 662, 1326, 746, 264, 1009, 47, 1061, 810, 78, 1063, 577]\n",
      "\n",
      "--- Performance Metrics ---\n",
      "Precision@50:   0.0600\n",
      "Recall@50:      0.1250\n",
      "F1-Score@50:    0.0811\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate_single_query(\n",
    "    query_id=2, \n",
    "    k=50, \n",
    "    queries=queries, \n",
    "    relevance_data=relevance_data, \n",
    "    document_vectors=document_vectors, \n",
    "    idf_scores=idf_scores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcf8b0",
   "metadata": {},
   "source": [
    "Evaluation with Pseudo-Relevance Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "129d3ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting full evaluation with Pseudo-Relevance Feedback... ---\n",
      "Evaluation complete. Processed 225 queries.\n"
     ]
    }
   ],
   "source": [
    "# Run Full System Evaluation with Pseudo-Relevance Feedback\n",
    "\n",
    "# import the updated functions \n",
    "from retrieval import preprocess_text, apply_rocchio_feedback,create_query_vector ,rank_documents\n",
    "\n",
    "all_metrics_feedback = []\n",
    "K = 10 # Evaluate Precision@10\n",
    "N_FEEDBACK_DOCS = 3 # Assume top 3 documents are relevant for feedback\n",
    "\n",
    "print(\"--- Starting full evaluation with Pseudo-Relevance Feedback... ---\")\n",
    "\n",
    "if document_vectors and queries and relevance_data:\n",
    "    for query_id, query_text in queries.items():\n",
    "        \n",
    "        # --- Initial Query ---\n",
    "        query_tokens = preprocess_text([query_text])[0]\n",
    "\n",
    "        original_query_vector = create_query_vector(query_tokens, idf_scores)\n",
    "\n",
    "        # --- Get Initial Ranking ---\n",
    "        initial_ranked_results = rank_documents(original_query_vector, document_vectors)\n",
    "        \n",
    "        # --- Apply Pseudo-Relevance Feedback ---\n",
    "        assumed_relevant_ids = [doc_id - 1 for doc_id, score in initial_ranked_results[:N_FEEDBACK_DOCS]]\n",
    "        \n",
    "        if not assumed_relevant_ids:\n",
    "            final_ranked_results = initial_ranked_results\n",
    "        else:\n",
    "            #  Pass the `idf_scores` dictionary to the feedback function.\n",
    "            modified_query_vector = apply_rocchio_feedback(\n",
    "                original_query_vector,\n",
    "                assumed_relevant_ids,\n",
    "                [], # No non-relevant documents\n",
    "                document_vectors,\n",
    "                idf_scores,\n",
    "                alpha=1.0, beta=0.75, gamma=0.0 \n",
    "            )\n",
    "            \n",
    "            # Re-rank the documents using the modified query vector\n",
    "            final_ranked_results = rank_documents(modified_query_vector, document_vectors)\n",
    "\n",
    "        # --- Calculate Metrics on the FINAL ranking ---\n",
    "        retrieved_top_k_ids = [doc_id for doc_id, score in final_ranked_results[:K]]\n",
    "        ground_truth_ids = relevance_data.get(query_id, [])\n",
    "        query_metrics = calculate_metrics(retrieved_top_k_ids, ground_truth_ids)\n",
    "        all_metrics_feedback.append(query_metrics)\n",
    "\n",
    "print(f\"Evaluation complete. Processed {len(all_metrics_feedback)} queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd4341a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- System Performance Summary ---\n",
      "------------------------------\n",
      "Mean Precision: 0.0049\n",
      "Mean Recall:    0.0029\n",
      "Mean F1-Score:  0.0035\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check if the evaluation loop has produced any metrics\n",
    "if all_metrics_feedback:\n",
    "    # Calculate the average (mean) of each metric across all 225 queries\n",
    "    mean_precision = sum(m['precision'] for m in all_metrics) / len(all_metrics)\n",
    "    mean_recall = sum(m['recall'] for m in all_metrics) / len(all_metrics)\n",
    "    mean_f1_score = sum(m['f1_score'] for m in all_metrics) / len(all_metrics)\n",
    "\n",
    "    # Print a clean, formatted summary of the final results\n",
    "    print(\"\\n--- System Performance Summary ---\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall:    {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1-Score:  {mean_f1_score:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"No metrics were calculated. Please ensure the evaluation loop ran correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ecb39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def evaluate_single_query_with_feedback(query_id, k, n_feedback, queries, relevance_data, document_vectors, idf_scores):\n",
    "    \"\"\"\n",
    "    Runs the full retrieval and evaluation process for a single query ID,\n",
    "    including a pseudo-relevance feedback step, and prints a detailed summary.\n",
    "    \"\"\"\n",
    "    # --- Check if the query exists ---\n",
    "    if query_id not in queries:\n",
    "        print(f\"Error: Query ID {query_id} not found.\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Get Query and Ground Truth ---\n",
    "    query_text = queries[query_id]\n",
    "    ground_truth_ids = relevance_data.get(query_id, [])\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"EVALUATING QUERY ID: {query_id}\")\n",
    "    print(f\"Query Text: '{query_text}'\")\n",
    "    print(f\"Ground Truth Relevant Docs: {ground_truth_ids}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # --- 2. Initial Search (Before Feedback) --- \n",
    "    query_tokens = preprocess_text([query_text])[0]\n",
    "    original_query_vector = create_query_vector(query_tokens, idf_scores)\n",
    "    initial_ranked_results = rank_documents(original_query_vector, document_vectors)\n",
    "\n",
    "    print(\"\\n--- Initial Top 5 Results (Before Feedback) ---\")\n",
    "    for rank, (doc_id, score) in enumerate(initial_ranked_results[:5], 1):\n",
    "        print(f\"Rank {rank}: Doc {doc_id} (Score: {score:.4f})\")\n",
    "    \n",
    "    # --- 3. Apply Pseudo-Relevance Feedback ---\n",
    "    print(f\"\\n--- Applying Pseudo-Relevance Feedback (using top {n_feedback} docs) ---\")\n",
    "    assumed_relevant_ids = [doc_id - 1 for doc_id, score in initial_ranked_results[:n_feedback]]\n",
    "    \n",
    "    if not assumed_relevant_ids:\n",
    "        print(\"No initial results found, skipping feedback.\")\n",
    "        final_ranked_results = initial_ranked_results\n",
    "    else:\n",
    "        # Create the new, improved query vector using the Rocchio formula\n",
    "        modified_query_vector = apply_rocchio_feedback(\n",
    "            original_query_vector,\n",
    "            assumed_relevant_ids,\n",
    "            [], # No non-relevant documents for pseudo-feedback\n",
    "            document_vectors,\n",
    "            idf_scores,\n",
    "            alpha=1.0, beta=0.75, gamma=0.0 # Gamma is 0\n",
    "        )\n",
    "        \n",
    "        # **THE CRITICAL FIX:** Re-normalize the modified query vector to a length of 1.\n",
    "        magnitude_squared = sum(weight**2 for weight in modified_query_vector.values())\n",
    "        magnitude = math.sqrt(magnitude_squared)\n",
    "        \n",
    "        final_query_vector = {}\n",
    "        if magnitude > 0:\n",
    "            for term, weight in modified_query_vector.items():\n",
    "                final_query_vector[term] = weight / magnitude\n",
    "\n",
    "        # Re-rank the documents using the properly normalized modified query vector\n",
    "        final_ranked_results = rank_documents(final_query_vector, document_vectors)\n",
    "\n",
    "    # --- 4. Final Results and Metrics (After Feedback) ---\n",
    "    print(\"\\n--- Final Top 5 Results (After Feedback) ---\")\n",
    "    if not final_ranked_results:\n",
    "        print(\"No documents found after feedback.\")\n",
    "    else:\n",
    "        for rank, (doc_id, score) in enumerate(final_ranked_results[:k], 1):\n",
    "            print(f\"Rank {rank}: Doc {doc_id} (Score: {score:.4f})\")\n",
    "    \n",
    "    retrieved_top_k_ids = [doc_id for doc_id, score in final_ranked_results[:k]]\n",
    "    metrics = calculate_metrics(retrieved_top_k_ids, ground_truth_ids)\n",
    "\n",
    "    print(\"\\n--- Final Performance Metrics ---\")\n",
    "    print(f\"Precision@{k}:   {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall@{k}:      {metrics['recall']:.4f}\")\n",
    "    print(f\"F1-Score@{k}:    {metrics['f1_score']:.4f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97f71710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING QUERY ID: 2\n",
      "Query Text: 'what are the structural and aeroelastic problems associated with flight\n",
      "of high speed aircraft .'\n",
      "Ground Truth Relevant Docs: [12, 15, 184, 858, 51, 102, 202, 14, 52, 380, 746, 859, 948, 285, 390, 391, 442, 497, 643, 856, 857, 877, 864, 658]\n",
      "============================================================\n",
      "\n",
      "--- Initial Top 5 Results (Before Feedback) ---\n",
      "Rank 1: Doc 12 (Score: 0.3549)\n",
      "Rank 2: Doc 1167 (Score: 0.3457)\n",
      "Rank 3: Doc 51 (Score: 0.3386)\n",
      "Rank 4: Doc 253 (Score: 0.3014)\n",
      "Rank 5: Doc 791 (Score: 0.2656)\n",
      "\n",
      "--- Applying Pseudo-Relevance Feedback (using top 3 docs) ---\n",
      "\n",
      "--- Final Top 5 Results (After Feedback) ---\n",
      "Rank 1: Doc 51 (Score: 0.7433)\n",
      "Rank 2: Doc 1167 (Score: 0.6496)\n",
      "Rank 3: Doc 12 (Score: 0.5902)\n",
      "Rank 4: Doc 253 (Score: 0.2792)\n",
      "Rank 5: Doc 1161 (Score: 0.2715)\n",
      "Rank 6: Doc 1168 (Score: 0.2636)\n",
      "Rank 7: Doc 791 (Score: 0.2629)\n",
      "Rank 8: Doc 883 (Score: 0.2363)\n",
      "Rank 9: Doc 882 (Score: 0.2141)\n",
      "Rank 10: Doc 496 (Score: 0.2132)\n",
      "Rank 11: Doc 1142 (Score: 0.2131)\n",
      "Rank 12: Doc 1087 (Score: 0.2057)\n",
      "Rank 13: Doc 924 (Score: 0.2021)\n",
      "Rank 14: Doc 605 (Score: 0.2019)\n",
      "Rank 15: Doc 47 (Score: 0.1995)\n",
      "Rank 16: Doc 725 (Score: 0.1952)\n",
      "Rank 17: Doc 100 (Score: 0.1943)\n",
      "Rank 18: Doc 809 (Score: 0.1870)\n",
      "Rank 19: Doc 1163 (Score: 0.1814)\n",
      "Rank 20: Doc 29 (Score: 0.1738)\n",
      "Rank 21: Doc 1166 (Score: 0.1721)\n",
      "Rank 22: Doc 724 (Score: 0.1681)\n",
      "Rank 23: Doc 908 (Score: 0.1639)\n",
      "Rank 24: Doc 141 (Score: 0.1601)\n",
      "Rank 25: Doc 1261 (Score: 0.1558)\n",
      "Rank 26: Doc 1165 (Score: 0.1548)\n",
      "Rank 27: Doc 1164 (Score: 0.1533)\n",
      "Rank 28: Doc 102 (Score: 0.1502)\n",
      "Rank 29: Doc 1359 (Score: 0.1473)\n",
      "Rank 30: Doc 745 (Score: 0.1467)\n",
      "Rank 31: Doc 75 (Score: 0.1461)\n",
      "Rank 32: Doc 1377 (Score: 0.1429)\n",
      "Rank 33: Doc 803 (Score: 0.1423)\n",
      "Rank 34: Doc 202 (Score: 0.1422)\n",
      "Rank 35: Doc 1195 (Score: 0.1408)\n",
      "Rank 36: Doc 78 (Score: 0.1405)\n",
      "Rank 37: Doc 1040 (Score: 0.1376)\n",
      "Rank 38: Doc 723 (Score: 0.1362)\n",
      "Rank 39: Doc 453 (Score: 0.1360)\n",
      "Rank 40: Doc 1160 (Score: 0.1357)\n",
      "Rank 41: Doc 415 (Score: 0.1354)\n",
      "Rank 42: Doc 184 (Score: 0.1337)\n",
      "Rank 43: Doc 1326 (Score: 0.1330)\n",
      "Rank 44: Doc 835 (Score: 0.1327)\n",
      "Rank 45: Doc 395 (Score: 0.1291)\n",
      "Rank 46: Doc 1145 (Score: 0.1288)\n",
      "Rank 47: Doc 746 (Score: 0.1287)\n",
      "Rank 48: Doc 859 (Score: 0.1278)\n",
      "Rank 49: Doc 1378 (Score: 0.1273)\n",
      "Rank 50: Doc 699 (Score: 0.1270)\n",
      "\n",
      "--- Final Performance Metrics ---\n",
      "Precision@50:   0.1400\n",
      "Recall@50:      0.2917\n",
      "F1-Score@50:    0.1892\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_query_with_feedback(\n",
    "    query_id=2, \n",
    "    k=50, \n",
    "    n_feedback=3,\n",
    "    queries=queries, \n",
    "    relevance_data=relevance_data, \n",
    "    document_vectors=document_vectors,\n",
    "    idf_scores=idf_scores\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
