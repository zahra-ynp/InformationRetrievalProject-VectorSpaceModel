{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56547735",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87d0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(filepath):\n",
    "\n",
    "    \"\"\"\n",
    "    Parses the Cranfield dataset from the given file.\n",
    "\n",
    "    This function reads the file, identifies the documents based on the '.I' marker,\n",
    "    and extracts text from the '.T' (Title) and '.W' (Words) fields.\n",
    "    \n",
    "    Returns:\n",
    "         A list of strings, where each string is the raw, unprocessed text of a document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to hold the raw text of all documents.\n",
    "    documents_raw = []\n",
    "\n",
    "    # A variable to hold the text of the current document being processed.\n",
    "    current_text = \"\"\n",
    "    \n",
    "    # A boolean flag to track if the current line is part of a text field (.T or .W).\n",
    "    is_text_section = False\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        # Iterate through each line in the file.\n",
    "        for line in f:\n",
    "            # A line starting with '.I' marks the beginning of a new document.\n",
    "            if line.startswith('.I'):\n",
    "                # If 'current_text' is not empty, it means we have finished reading a document.\n",
    "                if current_text:\n",
    "                    # Append the complete text of the previous document to our list.\n",
    "                    documents_raw.append(current_text.strip())\n",
    "                \n",
    "                # Reset 'current_text' to start for the new document.\n",
    "                current_text = \"\"\n",
    "                # Reset the flag, as we don't know what the next section will be.\n",
    "                is_text_section = False\n",
    "\n",
    "            # If a line starts with '.T' or '.W', it's a section we want to capture.\n",
    "            elif line.startswith(('.T', '.W')):\n",
    "                # Set our flag to True to start accumulating text from this and subsequent lines.\n",
    "                is_text_section = True\n",
    "            \n",
    "            # If a line starts with '.A' or '.B', it's metadata we want to ignore.\n",
    "            elif line.startswith(('.A', '.B')):\n",
    "                # Set our flag to False to stop accumulating text until we see a new .T or .W.\n",
    "                is_text_section = False\n",
    "                \n",
    "            # If the line doesn't start with a marker AND our flag is True...\n",
    "            elif is_text_section:\n",
    "                # ...it's a continuation of a title or abstract, so append it.\n",
    "                # We add a space to ensure words from different lines are not merged together.\n",
    "                current_text += line.strip() + \" \"\n",
    "\n",
    "    # After the loop finishes, the last document's text is still held in 'current_text'.\n",
    "    # This final check ensures the very last document in the file is added to the list.\n",
    "    if current_text:\n",
    "        documents_raw.append(current_text.strip())\n",
    "\n",
    "    # A confirmation message for loading\n",
    "    print(f\"Successfully loaded {len(documents_raw)} raw documents.\")\n",
    "    \n",
    "    # Return the final list\n",
    "    return documents_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae641e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1398 raw documents.\n"
     ]
    }
   ],
   "source": [
    "articles = import_dataset('./Dataset/cran.all.1400')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4d525",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ffab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# You only need to run these lines once to download the necessary NLTK packages.\n",
    "# try:\n",
    "#     stopwords.words('english')\n",
    "# except LookupError:\n",
    "#     nltk.download('stopwords')\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except LookupError:\n",
    "#     nltk.download('punkt')\n",
    "# try:\n",
    "#     nltk.data.find('corpora/wordnet')\n",
    "# except LookupError:\n",
    "#     nltk.download('wordnet')\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd86763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(raw_docs, method='lemmatize'):\n",
    "    \"\"\"\n",
    "    Takes a list of raw document strings and applies all preprocessing steps.\n",
    "\n",
    "    Args:\n",
    "        raw_docs (list of str): The list of unprocessed document texts.\n",
    "        method (str): The word reduction method to use. Can be 'lemmatize' (default)\n",
    "                      or 'stem'.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains the processed tokens\n",
    "        of a single document.\n",
    "    \"\"\"\n",
    "    # Initialize lists and objects for preprocessing.\n",
    "    processed_docs = []\n",
    "    \n",
    "    # 1. TOKENIZATION AND NORMALIZATION (LOWERCASE, PUNCTUATION REMOVAL)\n",
    "    # The tokenizer will split the document text into a list of words.\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # 2. STOP WORD REMOVAL\n",
    "    # Load the set of English stop words. Using a set provides fast lookups.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 3. STEMMING / LEMMATIZATION\n",
    "    # Initialize the chosen processor.\n",
    "    if method == 'lemmatize':\n",
    "        processor = WordNetLemmatizer()\n",
    "        process_func = processor.lemmatize\n",
    "    elif method == 'stem':\n",
    "        processor = PorterStemmer()\n",
    "        process_func = processor.stem\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'lemmatize' or 'stem'\")\n",
    "\n",
    "    # Process each document in the input list.\n",
    "    for doc in raw_docs:\n",
    "        # Lowercase the document text.\n",
    "        doc = doc.lower()\n",
    "        \n",
    "        # Use the tokenizer to get a list of alphabetic tokens.\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        \n",
    "        # Filter out stop words from the token list.\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # Apply the chosen processing (lemmatization or stemming) to each token.\n",
    "        processed_tokens = [process_func(token) for token in filtered_tokens]\n",
    "        \n",
    "        # Add the final list of processed tokens to our main list.\n",
    "        processed_docs.append(processed_tokens)\n",
    "        \n",
    "    print(f\"Finished preprocessing all documents using the '{method}' method.\")\n",
    "    return processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af9f56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing all documents using the 'lemmatize' method.\n",
      "\n",
      "--- Verification Sample (Document #2) ---\n",
      "\n",
      "[Raw Text]:\n",
      "simple shear flow past a flat plate in an incompressible fluid of small viscosity . simple shear flow past a flat plate in an incompressible fluid of small viscosity . in the study of high-speed viscous flow past a two-dimensional body it is usually necessary to consider a curved shock wave emitting from the nose or leading edge of the body .  consequently, there exists an inviscid rotational flow region between the shock wave and the boundary layer .  such a situation arises, for instance, in the study of the hypersonic viscous flow past a flat plate .  the situation is somewhat different from prandtl's classical boundary-layer problem . in prandtl's original problem the inviscid free stream outside the boundary layer is irrotational while in a hypersonic boundary-layer problem the inviscid free stream must be considered as rotational .  the possible effects of vorticity have been recently discussed by ferri and libby .  in the present paper, the simple shear flow past a flat plate in a fluid of small viscosity is investigated .  it can be shown that this problem can again be treated by the boundary-layer approximation, the only novel feature being that the free stream has a constant vorticity .  the discussion here is restricted to two-dimensional incompressible steady flow .\n",
      "\n",
      "[Processed Tokens]:\n",
      "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', 'study', 'high', 'speed', 'viscous', 'flow', 'past', 'two', 'dimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', 'consequently', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', 'situation', 'arises', 'instance', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', 'situation', 'somewhat', 'different', 'prandtl', 'classical', 'boundary', 'layer', 'problem', 'prandtl', 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundary', 'layer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', 'possible', 'effect', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', 'present', 'paper', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', 'shown', 'problem', 'treated', 'boundary', 'layer', 'approximation', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', 'discussion', 'restricted', 'two', 'dimensional', 'incompressible', 'steady', 'flow']\n"
     ]
    }
   ],
   "source": [
    "processed_articles = preprocess_text(articles)\n",
    "        \n",
    "# --- Display a sample to verify the process ---\n",
    "print(\"\\n--- Verification Sample (Document #2) ---\")\n",
    "        \n",
    "# Print the raw text of the second document (index 1)\n",
    "print(\"\\n[Raw Text]:\")\n",
    "print(articles[1])\n",
    "        \n",
    "# Print the same document after preprocessing\n",
    "print(\"\\n[Processed Tokens]:\")\n",
    "print(processed_articles[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa6c9d",
   "metadata": {},
   "source": [
    "Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def make_positional_index(processed_docs):\n",
    "    \"\"\"\n",
    "    Builds a positional inverted index from the processed documents.\n",
    "\n",
    "    The index is a dictionary where keys are terms. The value for each term\n",
    "    is another dictionary, where keys are document IDs and values are lists\n",
    "    of the positions where the term appears in that document.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Indexing ---\")\n",
    "    \n",
    "    # We use a defaultdict of dicts to easily create nested structures.\n",
    "    # If a term is new, it will automatically be assigned an empty dictionary.\n",
    "    p_index = defaultdict(dict)\n",
    "\n",
    "    # Enumerate over the processed documents to get both the document ID (docid)\n",
    "    # and the list of tokens for that document. The index acts as the docID.\n",
    "    for docid, tokens in enumerate(processed_docs):\n",
    "        # Enumerate over the tokens in the current document to get both the\n",
    "        # position (pos) and the term itself.\n",
    "        for pos, term in enumerate(tokens):\n",
    "            # Check if the docid is already a key for this term.\n",
    "            if docid in p_index[term]:\n",
    "                # Append the new position to the existing list.\n",
    "                p_index[term][docid].append(pos)\n",
    "            else:\n",
    "                # If the docid is not a key, this is the first time the term\n",
    "                # Create a new list containing the current position.\n",
    "                p_index[term][docid] = [pos]\n",
    "\n",
    "    print(f\"Finished indexing. The vocabulary contains {len(p_index)} unique terms.\")\n",
    "    return p_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44174561",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_index = make_positional_index(processed_articles)\n",
    "\n",
    "# --- Display a sample of the positional index ---\n",
    "\n",
    "for i, (term, postings) in enumerate(p_index.items()):\n",
    "    if i >= 2:  # Limit to first 2 terms for brevity\n",
    "        break\n",
    "    print(f\"Term: '{term}'\")\n",
    "    for docid, positions in postings.items():\n",
    "        print(f\"  DocID: {docid}, Positions: {positions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae464fb2",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idf_scores = {}\n",
    "\n",
    "\n",
    "def create_document_vectors(positional_index, total_docs):\n",
    "    \"\"\"\n",
    "    Creates TF-IDF vectors for each document in the collection.\n",
    "\n",
    "    The function calculates a TF-IDF weight for every term in the vocabulary\n",
    "    for each document. The resulting vectors are stored as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Document Vectors (TF-IDF) ---\")\n",
    "    \n",
    "    # Get the entire vocabulary from the keys of the positional index.\n",
    "    vocabulary = list(positional_index.keys())\n",
    "    \n",
    "    # --- Step 1: Calculate IDF for every term in the vocabulary ---\n",
    "    # The IDF score measures how important a term is.\n",
    "    for term in vocabulary:\n",
    "        # We can calculate df from the length of the dictionary for that term in the index.\n",
    "        df = len(positional_index[term])\n",
    "        # The IDF formula is log(N/df), where N is the total number of documents.\n",
    "        idf_scores[term] = math.log(total_docs / df)\n",
    "\n",
    "    # --- Step 2: Calculate TF-IDF for each term in each document ---\n",
    "    doc_vectors = []\n",
    "    # Iterate through each document\n",
    "    for doc_id in range(total_docs):\n",
    "        # Create a dictionary to hold the TF-IDF vector for the current document.\n",
    "        doc_vector = {}\n",
    "        \n",
    "        # Iterate through every term in the entire vocabulary.\n",
    "        for term in vocabulary:\n",
    "            # Check if the term exists in the current document.\n",
    "            if doc_id in positional_index[term]:\n",
    "                # We can calculate tf from the length of the positions list for that term/doc.\n",
    "                tf = len(positional_index[term][doc_id])\n",
    "                \n",
    "                # The TF-IDF weight is simply TF * IDF.\n",
    "                tfidf_weight = tf * idf_scores[term]\n",
    "                doc_vector[term] = tfidf_weight\n",
    "            else:\n",
    "                # If the term is not in the document, its TF-IDF weight is 0.\n",
    "                doc_vector[term] = 0.0\n",
    "                \n",
    "        # Add the completed vector for the current document to our list.\n",
    "        doc_vectors.append(doc_vector)\n",
    "\n",
    "    print(f\"Finished creating {len(doc_vectors)} document vectors.\")\n",
    "    return doc_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b77b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = len(articles)\n",
    "document_vectors = create_document_vectors(p_index, total_docs)\n",
    "\n",
    "print(\"\\n--- TF-IDF Vector for Document 1 ---\")\n",
    "doc1_scores = [(term, score) for term, score in document_vectors[0].items() if score > 0]\n",
    "doc1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "for term, score in doc1_scores[:10]:\n",
    "    print(f\"{term}:\\t{score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab31a35",
   "metadata": {},
   "source": [
    "Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(\"\\n--- Saving data to disk ---\")\n",
    "    \n",
    "# Define the file paths\n",
    "VECTORS_FILE = '.\\artifacts\\cranfield_vectors.pkl'\n",
    "IDF_FILE = '.\\artifacts\\cranfield_idf.pkl'\n",
    "RAW_DOCS_FILE = '.\\artifacts\\cranfield_raw_docs.pkl'\n",
    "\n",
    "#  Save the document vectors\n",
    "with open(VECTORS_FILE, 'wb') as f:\n",
    "    pickle.dump(document_vectors, f)\n",
    "\n",
    "# Save the IDF scores\n",
    "with open(IDF_FILE, 'wb') as f:\n",
    "    pickle.dump(idf_scores, f)\n",
    "        \n",
    "# Save the raw documents\n",
    "with open(RAW_DOCS_FILE, 'wb') as f:\n",
    "    pickle.dump(articles, f)\n",
    "        \n",
    "print(f\"Saved document vectors to {VECTORS_FILE}\")\n",
    "print(f\"Saved IDF scores to {IDF_FILE}\")\n",
    "print(f\"Saved raw documents to {RAW_DOCS_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
