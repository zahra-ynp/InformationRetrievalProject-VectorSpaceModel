{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56547735",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d87d0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(filepath):\n",
    "\n",
    "    \"\"\"\n",
    "    Parses the Cranfield dataset from the given file.\n",
    "\n",
    "    This function reads the file, identifies the documents based on the '.I' marker,\n",
    "    and extracts text from the '.T' (Title) and '.W' (Words) fields.\n",
    "    \n",
    "    Returns:\n",
    "         A list of strings, where each string is the raw, unprocessed text of a document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to hold the raw text of all documents.\n",
    "    documents_raw = []\n",
    "\n",
    "    # A variable to hold the text of the current document being processed.\n",
    "    current_text = \"\"\n",
    "    \n",
    "    # A boolean flag to track if the current line is part of a text field (.T or .W).\n",
    "    is_text_section = False\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        # Iterate through each line in the file.\n",
    "        for line in f:\n",
    "            # A line starting with '.I' marks the beginning of a new document.\n",
    "            if line.startswith('.I'):\n",
    "                # If 'current_text' is not empty, it means we have finished reading a document.\n",
    "                if current_text:\n",
    "                    # Append the complete text of the previous document to our list.\n",
    "                    documents_raw.append(current_text.strip())\n",
    "                \n",
    "                # Reset 'current_text' to start for the new document.\n",
    "                current_text = \"\"\n",
    "                # Reset the flag, as we don't know what the next section will be.\n",
    "                is_text_section = False\n",
    "\n",
    "            # If a line starts with '.T' or '.W', it's a section we want to capture.\n",
    "            elif line.startswith(('.T', '.W')):\n",
    "                # Set our flag to True to start accumulating text from this and subsequent lines.\n",
    "                is_text_section = True\n",
    "            \n",
    "            # If a line starts with '.A' or '.B', it's metadata we want to ignore.\n",
    "            elif line.startswith(('.A', '.B')):\n",
    "                # Set our flag to False to stop accumulating text until we see a new .T or .W.\n",
    "                is_text_section = False\n",
    "                \n",
    "            # If the line doesn't start with a marker AND our flag is True...\n",
    "            elif is_text_section:\n",
    "                # ...it's a continuation of a title or abstract, so append it.\n",
    "                # We add a space to ensure words from different lines are not merged together.\n",
    "                current_text += line.strip() + \" \"\n",
    "\n",
    "    # After the loop finishes, the last document's text is still held in 'current_text'.\n",
    "    # This final check ensures the very last document in the file is added to the list.\n",
    "    if current_text:\n",
    "        documents_raw.append(current_text.strip())\n",
    "\n",
    "    # A confirmation message for loading\n",
    "    print(f\"Successfully loaded {len(documents_raw)} raw documents.\")\n",
    "    \n",
    "    # Return the final list\n",
    "    return documents_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae641e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1398 raw documents.\n"
     ]
    }
   ],
   "source": [
    "articles = import_dataset('./Dataset/cran.all.1400')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4d525",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ffab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# You only need to run these lines once to download the necessary NLTK packages.\n",
    "# try:\n",
    "#     stopwords.words('english')\n",
    "# except LookupError:\n",
    "#     nltk.download('stopwords')\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except LookupError:\n",
    "#     nltk.download('punkt')\n",
    "# try:\n",
    "#     nltk.data.find('corpora/wordnet')\n",
    "# except LookupError:\n",
    "#     nltk.download('wordnet')\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd86763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(raw_docs, method='lemmatize'):\n",
    "    \"\"\"\n",
    "    Takes a list of raw document strings and applies all preprocessing steps.\n",
    "\n",
    "    Args:\n",
    "        raw_docs (list of str): The list of unprocessed document texts.\n",
    "        method (str): The word reduction method to use. Can be 'lemmatize' (default)\n",
    "                      or 'stem'.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains the processed tokens\n",
    "        of a single document.\n",
    "    \"\"\"\n",
    "    # Initialize lists and objects for preprocessing.\n",
    "    processed_docs = []\n",
    "    \n",
    "    # 1. TOKENIZATION AND NORMALIZATION (LOWERCASE, PUNCTUATION REMOVAL)\n",
    "    # The tokenizer will split the document text into a list of words.\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # 2. STOP WORD REMOVAL\n",
    "    # Load the set of English stop words. Using a set provides fast lookups.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 3. STEMMING / LEMMATIZATION\n",
    "    # Initialize the chosen processor.\n",
    "    if method == 'lemmatize':\n",
    "        processor = WordNetLemmatizer()\n",
    "        process_func = processor.lemmatize\n",
    "    elif method == 'stem':\n",
    "        processor = PorterStemmer()\n",
    "        process_func = processor.stem\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'lemmatize' or 'stem'\")\n",
    "\n",
    "    # Process each document in the input list.\n",
    "    for doc in raw_docs:\n",
    "        # Lowercase the document text.\n",
    "        doc = doc.lower()\n",
    "        \n",
    "        # Use the tokenizer to get a list of alphabetic tokens.\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        \n",
    "        # Filter out stop words from the token list.\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # Apply the chosen processing (lemmatization or stemming) to each token.\n",
    "        processed_tokens = [process_func(token) for token in filtered_tokens]\n",
    "        \n",
    "        # Add the final list of processed tokens to our main list.\n",
    "        processed_docs.append(processed_tokens)\n",
    "        \n",
    "    print(f\"Finished preprocessing all documents using the '{method}' method.\")\n",
    "    return processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8af9f56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing all documents using the 'lemmatize' method.\n",
      "\n",
      "--- Verification Sample (Document #2) ---\n",
      "\n",
      "[Raw Text]:\n",
      "simple shear flow past a flat plate in an incompressible fluid of small viscosity . simple shear flow past a flat plate in an incompressible fluid of small viscosity . in the study of high-speed viscous flow past a two-dimensional body it is usually necessary to consider a curved shock wave emitting from the nose or leading edge of the body .  consequently, there exists an inviscid rotational flow region between the shock wave and the boundary layer .  such a situation arises, for instance, in the study of the hypersonic viscous flow past a flat plate .  the situation is somewhat different from prandtl's classical boundary-layer problem . in prandtl's original problem the inviscid free stream outside the boundary layer is irrotational while in a hypersonic boundary-layer problem the inviscid free stream must be considered as rotational .  the possible effects of vorticity have been recently discussed by ferri and libby .  in the present paper, the simple shear flow past a flat plate in a fluid of small viscosity is investigated .  it can be shown that this problem can again be treated by the boundary-layer approximation, the only novel feature being that the free stream has a constant vorticity .  the discussion here is restricted to two-dimensional incompressible steady flow .\n",
      "\n",
      "[Processed Tokens]:\n",
      "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', 'study', 'high', 'speed', 'viscous', 'flow', 'past', 'two', 'dimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', 'consequently', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', 'situation', 'arises', 'instance', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', 'situation', 'somewhat', 'different', 'prandtl', 'classical', 'boundary', 'layer', 'problem', 'prandtl', 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundary', 'layer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', 'possible', 'effect', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', 'present', 'paper', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', 'shown', 'problem', 'treated', 'boundary', 'layer', 'approximation', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', 'discussion', 'restricted', 'two', 'dimensional', 'incompressible', 'steady', 'flow']\n"
     ]
    }
   ],
   "source": [
    "processed_articles = preprocess_text(articles)\n",
    "        \n",
    "# --- Display a sample to verify the process ---\n",
    "print(\"\\n--- Verification Sample (Document #2) ---\")\n",
    "        \n",
    "# Print the raw text of the second document (index 1)\n",
    "print(\"\\n[Raw Text]:\")\n",
    "print(articles[1])\n",
    "        \n",
    "# Print the same document after preprocessing\n",
    "print(\"\\n[Processed Tokens]:\")\n",
    "print(processed_articles[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
