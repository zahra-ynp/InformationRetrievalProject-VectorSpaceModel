{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56547735",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(filepath):\n",
    "\n",
    "    \"\"\"\n",
    "    Parses the Cranfield dataset from the given file.\n",
    "\n",
    "    This function reads the file, identifies the documents based on the '.I' marker,\n",
    "    and extracts text from the '.T' (Title) and '.W' (Words) fields.\n",
    "    \n",
    "    Returns:\n",
    "         A list of strings, where each string is the raw, unprocessed text of a document.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to hold the raw text of all documents.\n",
    "    documents_raw = []\n",
    "\n",
    "    # A variable to hold the text of the current document being processed.\n",
    "    current_text = \"\"\n",
    "    \n",
    "    # A boolean flag to track if the current line is part of a text field (.T or .W).\n",
    "    is_text_section = False\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        # Iterate through each line in the file.\n",
    "        for line in f:\n",
    "            # A line starting with '.I' marks the beginning of a new document.\n",
    "            if line.startswith('.I'):\n",
    "                # If 'current_text' is not empty, it means we have finished reading a document.\n",
    "                if current_text:\n",
    "                    # Append the complete text of the previous document to our list.\n",
    "                    documents_raw.append(current_text.strip())\n",
    "                \n",
    "                # Reset 'current_text' to start for the new document.\n",
    "                current_text = \"\"\n",
    "                # Reset the flag, as we don't know what the next section will be.\n",
    "                is_text_section = False\n",
    "\n",
    "            # If a line starts with '.T' or '.W', it's a section we want to capture.\n",
    "            elif line.startswith(('.T', '.W')):\n",
    "                # Set our flag to True to start accumulating text from this and subsequent lines.\n",
    "                is_text_section = True\n",
    "            \n",
    "            # If a line starts with '.A' or '.B', it's metadata we want to ignore.\n",
    "            elif line.startswith(('.A', '.B')):\n",
    "                # Set our flag to False to stop accumulating text until we see a new .T or .W.\n",
    "                is_text_section = False\n",
    "                \n",
    "            # If the line doesn't start with a marker AND our flag is True...\n",
    "            elif is_text_section:\n",
    "                # ...it's a continuation of a title or abstract, so append it.\n",
    "                # We add a space to ensure words from different lines are not merged together.\n",
    "                current_text += line.strip() + \" \"\n",
    "\n",
    "    # After the loop finishes, the last document's text is still held in 'current_text'.\n",
    "    # This final check ensures the very last document in the file is added to the list.\n",
    "    if current_text:\n",
    "        documents_raw.append(current_text.strip())\n",
    "\n",
    "    # A confirmation message for loading\n",
    "    print(f\"Successfully loaded {len(documents_raw)} raw documents.\")\n",
    "    \n",
    "    # Return the final list\n",
    "    return documents_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae641e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = import_dataset('./Dataset/cran.all.1400')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4d525",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ffab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# You only need to run these lines once to download the necessary NLTK packages.\n",
    "# try:\n",
    "#     stopwords.words('english')\n",
    "# except LookupError:\n",
    "#     nltk.download('stopwords')\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except LookupError:\n",
    "#     nltk.download('punkt')\n",
    "# try:\n",
    "#     nltk.data.find('corpora/wordnet')\n",
    "# except LookupError:\n",
    "#     nltk.download('wordnet')\n",
    "# -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(raw_docs, method='lemmatize'):\n",
    "    \"\"\"\n",
    "    Takes a list of raw document strings and applies all preprocessing steps.\n",
    "\n",
    "    Args:\n",
    "        raw_docs (list of str): The list of unprocessed document texts.\n",
    "        method (str): The word reduction method to use. Can be 'lemmatize' (default)\n",
    "                      or 'stem'.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each inner list contains the processed tokens\n",
    "        of a single document.\n",
    "    \"\"\"\n",
    "    # Initialize lists and objects for preprocessing.\n",
    "    processed_docs = []\n",
    "    \n",
    "    # 1. TOKENIZATION AND NORMALIZATION (LOWERCASE, PUNCTUATION REMOVAL)\n",
    "    # The tokenizer will split the document text into a list of words.\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # 2. STOP WORD REMOVAL\n",
    "    # Load the set of English stop words. Using a set provides fast lookups.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # 3. STEMMING / LEMMATIZATION\n",
    "    # Initialize the chosen processor.\n",
    "    if method == 'lemmatize':\n",
    "        processor = WordNetLemmatizer()\n",
    "        process_func = processor.lemmatize\n",
    "    elif method == 'stem':\n",
    "        processor = PorterStemmer()\n",
    "        process_func = processor.stem\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'lemmatize' or 'stem'\")\n",
    "\n",
    "    # Process each document in the input list.\n",
    "    for doc in raw_docs:\n",
    "        # Lowercase the document text.\n",
    "        doc = doc.lower()\n",
    "        \n",
    "        # Use the tokenizer to get a list of alphabetic tokens.\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        \n",
    "        # Filter out stop words from the token list.\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # Apply the chosen processing (lemmatization or stemming) to each token.\n",
    "        processed_tokens = [process_func(token) for token in filtered_tokens]\n",
    "        \n",
    "        # Add the final list of processed tokens to our main list.\n",
    "        processed_docs.append(processed_tokens)\n",
    "        \n",
    "    print(f\"Finished preprocessing all documents using the '{method}' method.\")\n",
    "    return processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_articles = preprocess_text(articles)\n",
    "        \n",
    "# --- Display a sample to verify the process ---\n",
    "print(\"\\n--- Verification Sample (Document #2) ---\")\n",
    "        \n",
    "# Print the raw text of the second document (index 1)\n",
    "print(\"\\n[Raw Text]:\")\n",
    "print(articles[1])\n",
    "        \n",
    "# Print the same document after preprocessing\n",
    "print(\"\\n[Processed Tokens]:\")\n",
    "print(processed_articles[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa6c9d",
   "metadata": {},
   "source": [
    "Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def make_positional_index(processed_docs):\n",
    "    \"\"\"\n",
    "    Builds a positional inverted index from the processed documents.\n",
    "\n",
    "    The index is a dictionary where keys are terms. The value for each term\n",
    "    is another dictionary, where keys are document IDs and values are lists\n",
    "    of the positions where the term appears in that document.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Indexing ---\")\n",
    "    \n",
    "    # We use a defaultdict of dicts to easily create nested structures.\n",
    "    # If a term is new, it will automatically be assigned an empty dictionary.\n",
    "    p_index = defaultdict(dict)\n",
    "\n",
    "    # Enumerate over the processed documents to get both the document ID (docid)\n",
    "    # and the list of tokens for that document. The index acts as the docID.\n",
    "    for docid, tokens in enumerate(processed_docs):\n",
    "        # Enumerate over the tokens in the current document to get both the\n",
    "        # position (pos) and the term itself.\n",
    "        for pos, term in enumerate(tokens):\n",
    "            # Check if the docid is already a key for this term.\n",
    "            if docid in p_index[term]:\n",
    "                # Append the new position to the existing list.\n",
    "                p_index[term][docid].append(pos)\n",
    "            else:\n",
    "                # If the docid is not a key, this is the first time the term\n",
    "                # Create a new list containing the current position.\n",
    "                p_index[term][docid] = [pos]\n",
    "\n",
    "    print(f\"Finished indexing. The vocabulary contains {len(p_index)} unique terms.\")\n",
    "    return p_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44174561",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_index = make_positional_index(processed_articles)\n",
    "\n",
    "# --- Display a sample of the positional index ---\n",
    "\n",
    "for i, (term, postings) in enumerate(p_index.items()):\n",
    "    if i >= 2:  # Limit to first 2 terms for brevity\n",
    "        break\n",
    "    print(f\"Term: '{term}'\")\n",
    "    for docid, positions in postings.items():\n",
    "        print(f\"  DocID: {docid}, Positions: {positions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae464fb2",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def create_document_vectors(positional_index, total_docs):\n",
    "    \"\"\"\n",
    "    Creates TF-IDF vectors for each document in the collection.\n",
    "\n",
    "    The function calculates a TF-IDF weight for every term in the vocabulary\n",
    "    for each document. The resulting vectors are stored as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Document Vectors (TF-IDF) ---\")\n",
    "    \n",
    "    # Get the entire vocabulary from the keys of the positional index.\n",
    "    vocabulary = list(positional_index.keys())\n",
    "    \n",
    "    # --- Step 1: Calculate IDF for every term in the vocabulary ---\n",
    "    # The IDF score measures how important a term is.\n",
    "    idf_scores = {}\n",
    "    for term in vocabulary:\n",
    "        # We can calculate df from the length of the dictionary for that term in the index.\n",
    "        df = len(positional_index[term])\n",
    "        # The IDF formula is log(N/df), where N is the total number of documents.\n",
    "        idf_scores[term] = math.log(total_docs / df)\n",
    "\n",
    "    # --- Step 2: Calculate TF-IDF for each term in each document ---\n",
    "    doc_vectors = []\n",
    "    # Iterate through each document\n",
    "    for doc_id in range(total_docs):\n",
    "        # Create a dictionary to hold the TF-IDF vector for the current document.\n",
    "        doc_vector = {}\n",
    "        \n",
    "        # Iterate through every term in the entire vocabulary.\n",
    "        for term in vocabulary:\n",
    "            # Check if the term exists in the current document.\n",
    "            if doc_id in positional_index[term]:\n",
    "                # We can calculate tf from the length of the positions list for that term/doc.\n",
    "                tf = len(positional_index[term][doc_id])\n",
    "                \n",
    "                # The TF-IDF weight is simply TF * IDF.\n",
    "                tfidf_weight = tf * idf_scores[term]\n",
    "                doc_vector[term] = tfidf_weight\n",
    "            else:\n",
    "                # If the term is not in the document, its TF-IDF weight is 0.\n",
    "                doc_vector[term] = 0.0\n",
    "                \n",
    "        # Add the completed vector for the current document to our list.\n",
    "        doc_vectors.append(doc_vector)\n",
    "\n",
    "    print(f\"Finished creating {len(doc_vectors)} document vectors.\")\n",
    "    return doc_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b77b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating Document Vectors (TF-IDF) ---\n",
      "Finished creating 1398 document vectors.\n",
      "\n",
      "--- TF-IDF Vector for Document 1 ---\n",
      "slipstream:\t27.2085\n",
      "destalling:\t19.6490\n",
      "increment:\t11.2667\n",
      "lift:\t9.3798\n",
      "aerodynamics:\t8.1295\n",
      "evaluation:\t7.9694\n",
      "different:\t7.7381\n",
      "wing:\t7.3246\n",
      "subtracting:\t6.5497\n",
      "comparative:\t5.4510\n"
     ]
    }
   ],
   "source": [
    "total_docs = len(articles)\n",
    "document_vectors = create_document_vectors(p_index, total_docs)\n",
    "\n",
    "print(\"\\n--- TF-IDF Vector for Document 1 ---\")\n",
    "doc1_scores = [(term, score) for term, score in document_vectors[0].items() if score > 0]\n",
    "doc1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "for term, score in doc1_scores[:10]:\n",
    "    print(f\"{term}:\\t{score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
